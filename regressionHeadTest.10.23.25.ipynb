{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cdc324-548e-4a94-a218-6fcfb6654405",
   "metadata": {},
   "source": [
    "# Model Playground Testing\n",
    "I am trying to make sure my masked language model (MLM) works first before I try to do anything with changing the head especially to a predictive model head.\n",
    "\n",
    "This was last modified 10/23/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbe6a14-b615-4b7b-8fa8-635299fce4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at rafalposwiata/deproberta-large-v1 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9005\n",
      "\n",
      "text: I can’t focus on anything lately.\n",
      "score: 0.6301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DepRoBERTa large + predictive head (for depression scoring or whatever)\n",
    "# I’m just testing this version on my GPU (4070, CUDA 12.4)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "# load base model + tokenizer\n",
    "model_name = \"rafalposwiata/deproberta-large-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"model loaded on\", device)\n",
    "\n",
    "# attach a simple regression head\n",
    "class DepPredictor(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(base.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]  # CLS token\n",
    "        return self.fc(self.drop(cls))\n",
    "\n",
    "model = DepPredictor(base_model).to(device)\n",
    "\n",
    "# some dummy training samples (replace with your dataset later)\n",
    "texts = [\n",
    "    \"I feel sad and tired.\",\n",
    "    \"I’m doing okay today.\",\n",
    "    \"Everything feels meaningless.\",\n",
    "    \"I’m feeling better lately.\"\n",
    "]\n",
    "targets = [0.9, 0.2, 0.95, 0.3]  # just random values between 0 and 1\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, texts, y, tok, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.y = y\n",
    "        self.tok = tok\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": torch.tensor(self.y[i], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "data = TextData(texts, targets, tokenizer)\n",
    "loader = DataLoader(data, batch_size=2, shuffle=True)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# simple training loop\n",
    "model.train()\n",
    "for ep in range(3):\n",
    "    total = 0\n",
    "    for batch in tqdm(loader, desc=f\"epoch {ep+1}\"):\n",
    "        opt.zero_grad()\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        y = batch[\"target\"].unsqueeze(1).to(device)\n",
    "        pred = model(ids, mask)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(\"loss:\", round(total / len(loader), 4))\n",
    "\n",
    "# quick test\n",
    "model.eval()\n",
    "txt = \"I can’t focus on anything lately.\"\n",
    "x = tokenizer(txt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(x[\"input_ids\"], x[\"attention_mask\"]).item()\n",
    "print(f\"\\ntext: {txt}\\nscore: {pred:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b892f99-54a9-48d9-bde2-2f6060a9a7ea",
   "metadata": {},
   "source": [
    "# Revisions\n",
    "(added with cursor updates) --> \n",
    "\n",
    "1. Lowering learning rate for super small set from 2e-5\n",
    "2. Averaging patterns (~5 times) for stable outputs\n",
    "3. Clamping 0-1 output with sigmoid and changing losses (BCEloss) to reflect\n",
    "4. Tracks again the loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba2710f-acb4-4c9d-8191-03c7fd8d585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at rafalposwiata/deproberta-large-v1 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.2562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 loss: 0.0413\n",
      "text: I can’t focus on anything lately.\n",
      "predicted score: 0.1210\n",
      "\n",
      "text: Today was pretty good.\n",
      "predicted score: 0.6318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DepRoBERTa Large + stable predictive head\n",
    "# Lower LR, sigmoid output, averaged predictions, loss tracking\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device =\", device)\n",
    "\n",
    "# load base model + tokenizer\n",
    "model_name = \"rafalposwiata/deproberta-large-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# predictive head with sigmoid\n",
    "class DepPredictor(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.fc = nn.Linear(base.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # bound outputs between 0 and 1\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        val = self.fc(cls)\n",
    "        return self.sigmoid(val)\n",
    "\n",
    "model = DepPredictor(base_model).to(device)\n",
    "\n",
    "# tiny dataset example\n",
    "texts = [\n",
    "    \"I feel sad and tired.\",\n",
    "    \"I’m doing okay today.\",\n",
    "    \"Everything feels meaningless.\",\n",
    "    \"I’m feeling better lately.\"\n",
    "]\n",
    "targets = [0.95, 0.25, 0.99, 0.05]  # targets between 0 and 1\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, texts, y, tok, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.y = y\n",
    "        self.tok = tok\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": torch.tensor(self.y[i], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "data = TextData(texts, targets, tokenizer)\n",
    "loader = DataLoader(data, batch_size=2, shuffle=True)\n",
    "\n",
    "# optimizer + loss\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-5)  # smaller LR for stability\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# training loop with epoch loss tracking\n",
    "model.train()\n",
    "for ep in range(10): # more epochs to stabilize\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=f\"epoch {ep+1}\"):\n",
    "        opt.zero_grad()\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        y = batch[\"target\"].unsqueeze(1).to(device)\n",
    "        pred = model(ids, mask)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"epoch {ep+1} loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# inference — average predictions for stability\n",
    "model.eval()\n",
    "test_texts = [\n",
    "    \"I can’t focus on anything lately.\",\n",
    "    \"Today was pretty good.\"\n",
    "]\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for t in test_texts:\n",
    "        enc = tokenizer(t, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        outputs = [model(enc[\"input_ids\"], enc[\"attention_mask\"]) for _ in range(5)]  # run 5 times\n",
    "        avg = torch.mean(torch.stack(outputs))\n",
    "        preds.append(avg.item())\n",
    "\n",
    "for t, p in zip(test_texts, preds):\n",
    "    print(f\"text: {t}\\npredicted score: {p:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e189f-1637-482f-8031-8d9390ef212d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b050b56-0bb5-42b9-99dd-a68f0f428876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
